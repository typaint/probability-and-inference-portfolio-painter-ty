---
title: "Monte Carlo Error"
author: "Ty Painter"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document: 
    code_folding: hide
    toc: yes
    number_sections: true 
    toc_depth: 3
    toc_float: true
---
```{r global options, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(magrittr)
library(dplyr)
library(ggplot2)
set.seed(09102020)
```
 
# Introduction

  In a Monte Carlo simulation there is some degree of error in the quantity estimates made. I will analyze the relationship between the numbers of replicates run versus the simulation error by using a different set of parameters for each simualtion.

## Background

  The Monte Carlo simulation helps predict the **risk** or **uncertainty** of a situation by measuring the error of repeated simulations. Replicates and error have an inverse relationship, therefore as replicates in a simulation increase, the degree of error should decrease.
  There are two types of error that will be measured when estimating probabilities from simulation, **absolute error** and **relative error**. 
  
  **Absolute Error** = |Estimated Probability ($\hat{p}$) - True Probability (p)|
    The absolute error is the difference between the approximated (calculated in simulations) and actual probabilities.
    
  **Relative Error** = $\frac{|\hat{p}-p|} {p}$
    The relative error is the absolute error, relative to the actual probability.
  
  In this analysis I will run a 14x5 factorial simulation using a combination of replicate numbers ($2^2...2^{15}$) and probability (0.01, 0.05, 0.1, 0.25, 0.5) to estimate the errors above for each combination. The results will be plotted to display how the number of replicates will affect both types of errors.

# Methods

```{r}
# Simulation parameters
parameters <- list(ss = 4, # sample size
                   p = .1, # true probability 
                   r = 5000) # replicates

# Function to create data and estimate p_hat
create_data_estimate_p <- function(parameters) {
  parameters$p_hat <- rbinom(parameters$r, parameters$ss, parameters$p) / parameters$ss # rbinom creates data, then divide by sample size to estimate p_hat
  parameters
}

# Function to calculate absolute error = |p_hat - p|
absolute_error <- function(parameters) {
  abs(parameters$p_hat - parameters$p)
}

# Function to calculate mean absolute and relative error for each data point
one_p_n <- function(parameters) {
  ae<- parameters %>% create_data_estimate_p %>% absolute_error # assign absolute_error function to ae variable
    
  re <- ae / parameters$p # calculate relative error = |p_hat - p| / p
  mae <- mean(ae) # calculate mean absolute error
  mre <- mean(re) # calculate mean relative error
  c(mae, mre)
}

# Establish parameters for simulations
simulation_settings <- expand.grid(
    r = 5000
  , p = c(.01, .05, .1, .25, .5)
  , ss = 2^(2:15)
  , mae = NA_real_
  , mre = NA_real_
  , KEEP.OUT.ATTRS = FALSE
)

# For loop calculate the MAE and MRE for each row using the one_p_n function above
for(i in 1:nrow(simulation_settings)) {
  simulation_settings[i, c("mae", "mre")] <- simulation_settings[i, ] %>% as.list() %>% one_p_n 
}

# Display a summary of the data frame of the values that will be in the plots
summary(simulation_settings)
```
 
  This data frame displays the values of the mean absolute error (MAE) and mean relative error (MRE) for each combination of true probability (p) and sample size (ss) that will be used in the plots below. Each rows will represent its own data point on the plot.

# Results

```{r}
  ## Plots
require(tgsify)

simulation_settings %>% 
  mutate(col = factor(p) %>% as.numeric) %>% 
  plotstyle(upright, mar = c(3,3,2,1)) %>% 
  plot_setup(mae ~ log(ss, base = 2)) %>% 
  split(.$p) %>% 
  lwith({
    lines(log(ss, base = 2), mae, type = "b", col = col[1], lwd = 4)
    c(p[1], col[1])
  }) %>% 
  do.call("rbind", .) %>% 
  (function(x){
    legend("topright", legend = "p = "%|% x[,1], col =x[,2], lwd = 4, bty = "n")
    })
box()
axis(side = 1, at = axTicks(1), labels = 2^axTicks(1))
axis(2)
title(main = "Mean Absolute Error")
title(ylab = "MAE", line = 2)
title(xlab = "Sample Size", line = 1.5)


simulation_settings %>% 
  mutate(col = factor(p) %>% as.numeric) %>% 
  plotstyle(upright, mar = c(3,3,2,1)) %>% 
  plot_setup(mre ~ log(ss, base = 2)) %>% # change base, put error on log base (straight line relationships indicates linear structure)
  split(.$p) %>% 
  lwith({
    lines(log(ss, base = 2), mre, type = "b", col = col[1], lwd = 4) # change base
    c(p[1], col[1])
  }) %>% 
  do.call("rbind", .) %>% 
  (function(x){
    legend("topright", legend = "p = "%|% x[,1], col =x[,2], lwd = 4, bty = "n")
    })
box()
axis(side = 1, at = axTicks(1), labels = 2^axTicks(1)) # change base
axis(2)
title(main = "Mean Relative Error")
title(ylab = "MRE", line = 2)
title(xlab = "Sample Size", line = 1.5)
```
  
  The graphs above show the correlation between the MAE/MRE as the sample size increases. As expected both types of errors decrease and approach 0 as the sample size exponentially increases.

# Conclusions

  The one difference between mean absolute error and mean relative error is the correlation between the true probability. My interpretation of this is that for the for mean absolute error, the more extreme the probability (Ex: .01 or .99) the lower your mean absolute error because the amount of uncertainty is very low. On the other hand a probability of 0.5 leaves an uncertainty of half so the absolute error will be higher. 
 
  For mean relative error, the absolute error is compared to the actual probability. So the same amount of error for multiple probabilities of will be emphasized more will a smaller actual probability value (Ex: 01, .05), rather than the higher probabilities (Ex: .5). To put this into perspective, for the relative error to be equal for probability measurements of .01 and .5, the absolute error for a probability = .5 would have to be 50 times more than the absolute error of probability = .01.
